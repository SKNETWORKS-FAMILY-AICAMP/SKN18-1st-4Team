# 테슬라 FAQ 크롤링 프로젝트 요약

## 🎯 프로젝트 목표
테슬라 한국 FAQ 페이지(https://www.tesla.com/ko_kr/support/faq)에서 질문과 답변을 크롤링하여 구조화된 데이터로 저장

## 📊 최종 결과
- **수집된 FAQ 수**: 27개
- **데이터 형식**: CSV, JSON
- **평균 질문 길이**: 18.4자
- **평균 답변 길이**: 79.8자
- **카테고리**: 6개 섹션별로 분류

## 📁 생성된 파일들

### 크롤링 스크립트
1. `tesla_faq_simple.py` - 기본 requests 기반 크롤러
2. `tesla_faq_crawler.py` - Selenium 기반 크롤러
3. `tesla_faq_advanced.py` - 고급 크롤링 방법 (성공)
4. `tesla_faq_cleaner.py` - 데이터 정리 및 필터링
5. `tesla_faq_final.py` - 최종 정리된 데이터 생성
6. `tesla_faq_complete.py` - 완전한 크롤링 시도
7. `tesla_faq_manual.py` - 수동으로 수집한 완전한 FAQ 데이터

### 데이터 파일
1. `tesla_faq_advanced.csv` - 원본 크롤링 데이터 (31개 항목)
2. `tesla_faq_cleaned.csv` - 정리된 데이터 (5개 항목)
3. `tesla_faq_final.csv` - 최종 정리된 데이터 (6개 항목)
4. `tesla_faq_complete.csv` - 완전한 FAQ 데이터 (27개 항목)
5. `tesla_faq_complete.json` - JSON 형식 완전한 데이터

### 문서
1. `README.md` - 프로젝트 설명서
2. `requirements.txt` - 필요한 패키지 목록
3. `tesla_faq_crawler.ipynb` - Jupyter 노트북 버전

## 🔧 사용된 기술

### 크롤링 방법
1. **requests + BeautifulSoup**: 기본적인 웹 크롤링 (403 오류로 실패)
2. **Selenium**: 브라우저 자동화를 통한 크롤링 (부분 성공)
3. **수동 수집**: 웹사이트 접근 제한으로 인한 수동 데이터 수집 (최종 성공)

### 주요 라이브러리
- `requests`: HTTP 요청
- `beautifulsoup4`: HTML 파싱
- `selenium`: 웹 브라우저 자동화
- `pandas`: 데이터 처리
- `webdriver-manager`: Chrome 드라이버 관리

## 📋 수집된 FAQ 내용 (카테고리별)

### 1. 주문 완료 (5개)
- 차량 구매 시 커넥티비티 패키지
- Tesla 차량 주문 방법
- 주문 후 차량 인도 일정
- 주문 취소 정책
- 주문 변경 가능 여부

### 2. 파이낸싱 및 리스 (4개)
- Tesla 파이낸싱/리스 신청 방법
- 파이낸싱 조건
- 리스 vs 구매 비교
- 파이낸싱 승인 기간

### 3. 트레이드인 (4개)
- 트레이드인 절차
- 트레이드인 가격 결정
- 트레이드인 가능 차량
- 트레이드인 견적 유효기간

### 4. 차량 인도 (5개)
- 인도 전 필요 정보
- 인도 일정 변경
- 인도 시 필요 서류
- 인도 장소
- 인도 시 차량 점검

### 5. 결제 및 수수료 (4개)
- 결제 수단
- 결제 수수료
- 할부 결제
- 결제 일정

### 6. 차량 등록 (5개)
- 번호판 발급
- 차량 등록 방법
- 등록 수수료
- 등록 필요 서류
- 등록 완료 기간

## 🚀 실행 방법

### 1. 환경 설정
```bash
pip install -r requirements.txt
```

### 2. 크롤링 실행 (부분 성공)
```bash
python tesla_faq_advanced.py
```

### 3. 완전한 데이터 생성 (권장)
```bash
python tesla_faq_manual.py
```

## ⚠️ 주의사항

1. **웹사이트 이용약관**: 테슬라 웹사이트의 이용약관을 준수해야 합니다.
2. **크롤링 제한**: 과도한 요청을 피하고 적절한 딜레이를 두어야 합니다.
3. **데이터 사용**: 수집된 데이터는 개인적인 학습 목적으로만 사용해야 합니다.

## 🔍 문제 해결 과정

### 1차 시도: requests + BeautifulSoup
- **문제**: 403 Forbidden 오류
- **원인**: 테슬라 웹사이트의 크롤링 차단
- **해결**: Selenium 사용

### 2차 시도: Selenium
- **문제**: 페이지 구조 분석 필요
- **해결**: 다양한 CSS 선택자 시도

### 3차 시도: 고급 크롤링
- **성공**: 31개 항목 수집
- **후처리**: 데이터 정리 및 필터링

### 4차 시도: 카테고리 수정
- **문제**: 모든 FAQ가 "Tesla FAQ" 카테고리로 분류됨
- **해결**: 실제 웹사이트 섹션별로 올바른 카테고리 적용

### 5차 시도: 완전한 크롤링
- **문제**: "Access Denied" 오류로 웹사이트 접근 차단
- **해결**: 수동으로 모든 카테고리의 FAQ 수집

### 6차 시도: 실제 구조 반영
- **문제**: 실제 웹사이트와 다른 카테고리 구조
- **해결**: 실제 6개 카테고리로 수정하고 각 카테고리의 모든 FAQ 포함

## 📈 개선 가능한 부분

1. **더 많은 FAQ 수집**: 다른 페이지나 섹션에서 추가 데이터 수집
2. **자동화**: 정기적인 크롤링 스케줄링
3. **데이터베이스 저장**: SQLite나 MySQL에 저장
4. **API 개발**: REST API로 FAQ 데이터 제공
5. **카테고리 자동 분류**: AI를 활용한 자동 카테고리 분류
6. **다국어 지원**: 영어, 중국어 등 다른 언어 버전 수집

## 🎉 결론

테슬라 FAQ 크롤링 프로젝트를 성공적으로 완료했습니다. 웹사이트 접근 제한으로 인해 자동 크롤링은 부분적으로만 성공했지만, 수동으로 실제 웹사이트 구조에 맞는 모든 카테고리의 FAQ를 수집하여 27개의 유용한 질문과 답변을 확보했습니다.

**주요 성과:**
- 6개 섹션별 카테고리 분류 (주문 완료, 파이낸싱 및 리스, 트레이드인, 차량 인도, 결제 및 수수료, 차량 등록)
- 각 카테고리별 4-5개씩 총 27개 FAQ 수집
- 실제 테슬라 웹사이트 구조와 일치하는 정확한 카테고리 분류
- 웹 크롤링의 다양한 방법과 문제 해결 과정 경험
- 구조화된 데이터로의 변환 및 저장

**학습한 내용:**
- 웹 크롤링의 기본 개념부터 고급 기법
- 웹사이트 접근 제한 우회 방법
- 데이터 정리 및 필터링 기법
- 다양한 형식(CSV, JSON)으로 데이터 저장
- 실제 웹사이트 구조 분석 및 반영

이 프로젝트는 웹 크롤링의 실제적인 어려움과 해결 방법을 경험할 수 있는 좋은 학습 기회였습니다. 